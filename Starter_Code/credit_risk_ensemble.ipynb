{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "## Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CSV and Perform Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = Path('Resources/LoanStats_2019Q1.csv')\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features\n",
    "drop = df.drop('loan_status', axis=1)\n",
    "X = pd.get_dummies(drop)\n",
    "\n",
    "# Create our target\n",
    "y = df[['loan_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the balance of our target values\n",
    "# YOUR CODE HERE\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y into X_train, X_test, y_train, y_test\n",
    "# YOUR CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Scale the training and testing data using the `StandardScaler` from `sklearn`. Remember that when scaling the data, you only scale the features data (`X_train` and `X_testing`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler instance\n",
    "# YOUR CODE HERE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Standard Scaler with the training data\n",
    "# When fitting scaling functions, only train on the training dataset\n",
    "# YOUR CODE HERE\n",
    "X_scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the training and testing data\n",
    "# YOUR CODE HERE\n",
    "X_train_scaler = X_scaler.transform(X_train)\n",
    "X_test_scaler = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learners\n",
    "\n",
    "In this section, you will compare two ensemble algorithms to determine which algorithm results in the best performance. You will train a Balanced Random Forest Classifier and an Easy Ensemble classifier . For each algorithm, be sure to complete the folliowing steps:\n",
    "\n",
    "1. Train the model using the training data. \n",
    "2. Calculate the balanced accuracy score from sklearn.metrics.\n",
    "3. Display the confusion matrix from sklearn.metrics.\n",
    "4. Generate a classication report using the `imbalanced_classification_report` from imbalanced-learn.\n",
    "5. For the Balanced Random Forest Classifier only, print the feature importance sorted in descending order (most important feature to least important) along with the feature score\n",
    "\n",
    "Note: Use a random state of 1 for each algorithm to ensure consistency between tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training data with the BalancedRandomForestClassifier\n",
    "# YOUR CODE HERE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "random_forest = BalancedRandomForestClassifier(n_estimators=100, random_state=1)\n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated the balanced accuracy score\n",
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the imbalanced classification report\n",
    "# YOUR CODE HERE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the features sorted in descending order by feature importance\n",
    "# YOUR CODE HERE\n",
    "featured_names = pd.DataFrame(\n",
    "    random_forest.feature_importances_,\n",
    "    index = X_train.columns,\n",
    "    columns = ['Importance']).sort_values('Importance', ascending = False)\n",
    "featured_names.head(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Ensemble Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Classifier\n",
    "# YOUR CODE HERE\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "easy_ensemble = EasyEnsembleClassifier(n_estimators=100, random_state=1)\n",
    "easy_ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated the balanced accuracy score\n",
    "# YOUR CODE HERE\n",
    "y_pred = easy_ensemble.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "# YOUR CODE HERE\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the imbalanced classification report\n",
    "# YOUR CODE HERE\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Questions\n",
    "\n",
    "1. Which model had the best balanced accuracy score?\n",
    "\n",
    "    Of the two models, the Easy Ensemble had the better score.\n",
    "\n",
    "2. Which model had the best recall score?\n",
    "\n",
    "    Of the two models, the Easy Ensemble had the better score.\n",
    "\n",
    "3. Which model had the best geometric mean score?\n",
    "\n",
    "    Of the two models, the Easy Ensemble had the better score.\n",
    "\n",
    "4. What are the top three features?\n",
    "\n",
    "    See cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_three = featured_names.nlargest(3, 'Importance')\n",
    "best_three"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e8fbdd912121b231e02b6268cd60a0af7fc685a38550194c26e544dbe7165ed"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('dev': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
